# ğŸ§  AI Company Insight Chatbot (Local LLM + Semantic Search)

This project is a chatbot powered by LLaMA 2 and semantic search using FAISS. It allows users to ask questions about specific companies based on scraped content from their official websites.

---

## ğŸ“Œ Features

- âœ… Company-specific chatbot from scraped data
- âš™ï¸ Embedding via SentenceTransformer
- ğŸ” Semantic search with FAISS
- ğŸ¤– LLaMA 2 model served locally (`gguf`)
- ğŸ§  Automatic fallback to direct context if content is small
- ğŸ“¦ Streamlit interface


## ğŸ“ Project Structure

```
â”œâ”€â”€ data/ # Data & embedding files
â”‚ â”œâ”€â”€ scraped_content.txt # Text scraped from company website
â”‚ â”œâ”€â”€ faiss_index.index # FAISS index file
â”‚ â””â”€â”€ faiss_index_chunks.pkl # Embedded chunks (raw texts)
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ llama-2-7b-chat.Q4_K_S.gguf # Local LLaMA model
â”‚ â””â”€â”€ local_llm.py # LLM interface setup
â”‚
â”œâ”€â”€ pages/
â”‚ â””â”€â”€ detail_data.py # Streamlit detail & chat UI
â”‚
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ text_splitter.py # Text splitting utility
â”‚
â”œâ”€â”€ scrapper.py # Web content scraper
â”œâ”€â”€ embedder.py # Embedding and FAISS indexer
â”œâ”€â”€ main.py # Streamlit launcher
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This file
```

---

## ğŸš€ How to Run

### 1. Clone & install dependencies
```bash
git clone https://github.com/yourname/company-insight-chatbot.git
cd company-insight-chatbot
pip install -r requirements.txt
```
### 2. Download LLaMA 2 gguf model
Place your model in 
```
models/llama-2-7b-chat.Q4_K_S.gguf
```

You can find it on [Hugging Face](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q4_K_S.gguf)

### 3. Launch the app
```
streamlit run main.py
```
## ğŸ’¬ How It Works ###
Select a company

Click "Start Chat"

Scraper will extract website content (about, landing, etc.)

If content < 700 words â†’ LLM uses direct prompt stuffing

If content â‰¥ 700 words â†’ FAISS used to semantically search relevant chunks

LLaMA model answers your query strictly based on context

ğŸ”§ Requirements
All dependencies are listed in requirements.txt

## ğŸ“Œ Future Improvements
- Add UI to select from multiple companies
- Add cache & history
- Deploy via Docker for reproducibility
- Using "smarter" llm
- More advanced scraping techniques

ğŸ§‘â€ğŸ’» Author
@dzikrimaulana87